---
title: "Domain 3 - Applications of Foundation Models"
description: "AIF-C01 Domain 3: Model selection, prompt engineering, RAG, and cost optimization (28%)"
head:
  - - meta
    - name: keywords
      content: aif-c01, domain 3, prompt engineering, rag, model selection
---

# Domain 3: Applications of Foundation Models (28%)

[← Domain 2](./domain-2.md) · [Next Domain →](./domain-4.md)

---

## 3.1: Design Considerations

### Cost Optimization

**Factors Affecting Cost**:
- **Input tokens**: Text sent to model
- **Output tokens**: Text generated by model
- **Model size**: Larger models cost more
- **Request frequency**

**Cost Optimization Strategies**:
1. ✅ Choose appropriate model size
   - Don't use Claude Opus for simple tasks
2. ✅ Optimize prompts
   - Be concise, avoid unnecessary context
3. ✅ Cache responses
   - Store common queries
4. ✅ Use smaller models when possible
   - Titan for simple tasks, Claude for complex

### Latency Considerations

**Factors Affecting Latency**:
- Model size (larger = slower)
- Output length
- Context window usage
- Concurrent requests

**Optimization**:
- Use smaller models for real-time apps
- Implement streaming responses
- Asynchronous processing for batch jobs

---

## 3.2: Choosing the Right Foundation Model

<FlashcardDeck 
  title="Model Selection"
  :cards="[
    {
      question: 'Which model for long document analysis?',
      answer: '<strong>Anthropic Claude</strong><br>200K context window<br>Best for analyzing lengthy documents, contracts, research papers.'
    },
    {
      question: 'Which model for cost-effective text generation?',
      answer: '<strong>Amazon Titan Text</strong><br>AWS-optimized, lower cost<br>Best for simple chatbots, basic content generation.'
    },
    {
      question: 'Which model for image generation?',
      answer: '<strong>Stable Diffusion XL</strong><br>High-quality image generation<br>Available through Bedrock.'
    },
    {
      question: 'Which model for embeddings in RAG?',
      answer: '<strong>Amazon Titan Embeddings</strong><br>Optimized for semantic search<br>Best for RAG implementations.'
    }
  ]"
/>

### Decision Table: Model Selection

| Use Case | Recommended Model | Why |
|----------|-------------------|-----|
| Long document analysis | Anthropic Claude | 200K context window |
| Cost-effective text generation | Amazon Titan Text | AWS-optimized, lower cost |
| Image generation | Stable Diffusion XL | High-quality images |
| Code generation | Claude or CodeWhisperer | Strong coding abilities |
| Multilingual content | Cohere or Jurassic | Better language support |
| Embeddings for RAG | Amazon Titan Embeddings | Optimized for search |
| General chatbot | Claude or Titan | Good balance |

---

## 3.3: Prompt Engineering

<FlashcardDeck 
  title="Prompt Engineering Techniques"
  :cards="[
    {
      question: 'What is Zero-Shot Prompting?',
      answer: '<strong>No examples, just the instruction</strong><br>Example: &quot;Translate to French: Hello&quot;<br>Works well for common tasks.'
    },
    {
      question: 'What is Few-Shot Prompting?',
      answer: '<strong>Provide examples to guide the model</strong><br>Give 3-5 input/output examples<br>Best for: Custom formats, specific styles.'
    },
    {
      question: 'What is Chain-of-Thought Prompting?',
      answer: '<strong>Ask model to show its reasoning</strong><br>Add &quot;Show your work&quot; or &quot;Think step by step&quot;<br>Best for: Math, logic, complex reasoning.'
    }
  ]"
/>

### Zero-Shot Prompting
**No examples**, just the instruction

```
Prompt: "Translate to French: Hello, how are you?"
Output: "Bonjour, comment allez-vous?"
```

### Few-Shot Prompting
**Provide examples** to guide the model

```
Prompt:
Sentiment analysis examples:
"I love this!" → Positive
"Terrible experience" → Negative
"It was okay" → Neutral

Now classify: "Best purchase ever!"
Output: Positive
```

### Chain-of-Thought Prompting
Ask model to **show its reasoning**

```
Prompt: "If apples cost $2/lb and I buy 3.5 lbs, how much do I pay?
Show your work."

Output:
Let me calculate:
- Price per pound: $2
- Pounds purchased: 3.5
- Total: $2 × 3.5 = $7
Answer: $7
```

### Prompt Engineering Best Practices

1. ✅ **Be Specific**
   - ❌ "Write about dogs"
   - ✅ "Write a 200-word informative article about dog breeds suitable for apartment living"

2. ✅ **Provide Context**
   - ❌ "Summarize this"
   - ✅ "You are a financial analyst. Summarize this quarterly report for executives."

3. ✅ **Use Formatting**
   - Use bullet points, sections
   - Clear instructions

4. ✅ **Specify Output Format**
   - "Respond in JSON format"
   - "Use bullet points"

5. ✅ **Add Constraints**
   - "In 100 words or less"
   - "Using simple language"

---

## 3.4: Retrieval Augmented Generation (RAG)

### What is RAG?

**Problem**: LLMs don't know your private data or recent events

**Solution**: RAG provides relevant context to the model

<FlashcardDeck 
  title="RAG Concepts"
  :cards="[
    {
      question: 'What problem does RAG solve?',
      answer: '<strong>LLMs do not know your private data or recent events</strong><br>RAG retrieves relevant documents and provides them as context<br>Reduces hallucinations, enables current data access.'
    },
    {
      question: 'What are Embeddings in RAG?',
      answer: '<strong>Numerical representations of text</strong><br>Convert text to vectors for semantic search<br>Similar meanings = similar vectors.'
    },
    {
      question: 'What is a Vector Database?',
      answer: '<strong>Database for storing and searching embeddings</strong><br>Enables semantic search (meaning-based, not keyword)<br>AWS Options: OpenSearch, Bedrock Knowledge Bases.'
    }
  ]"
/>

**How RAG Works**:

```
1. User asks question
   ↓
2. Convert question to embedding (vector)
   ↓
3. Search vector database for similar content
   ↓
4. Retrieve relevant documents
   ↓
5. Provide documents + question to LLM
   ↓
6. LLM generates answer with context
```

### RAG Architecture Components

**1. Document Ingestion**
- Split documents into chunks
- Generate embeddings for each chunk
- Store in vector database

**2. Vector Database**
- Stores embeddings
- Enables semantic search
- AWS Options: OpenSearch, Bedrock Knowledge Bases

**3. Retrieval**
- Convert query to embedding
- Find similar documents
- Return top K results

**4. Generation**
- Send retrieved docs + query to LLM
- LLM generates contextual answer

### Benefits of RAG

✅ **Reduces Hallucinations**
- Grounds responses in factual documents

✅ **Up-to-Date Information**
- Access to current data

✅ **Domain-Specific Knowledge**
- Use your proprietary data

✅ **Citations**
- Can reference source documents

✅ **Cost-Effective**
- No fine-tuning needed

### Amazon Bedrock Knowledge Bases

**Managed RAG solution**

**Features**:
- Automatic document ingestion
- Vector database management
- Built-in embeddings (Titan Embeddings)
- Simple API integration

**Supported Data Sources**:
- Amazon S3
- Web crawlers
- Confluence
- SharePoint
- Salesforce

---

### Fine-Tuning vs. Prompt Engineering

| Approach | When to Use | Cost | Effort | Flexibility |
|----------|-------------|------|--------|-------------|
| **Prompt Engineering** | First choice | Low | Low | High |
| **RAG** | Need current/private data | Medium | Medium | High |
| **Fine-Tuning** | Domain-specific tasks | High | High | Low |

::: tip Exam Decision Pattern
- **Start with prompt engineering** (cheapest, fastest)
- **Use RAG** if you need external knowledge
- **Fine-tune** only if above methods insufficient
:::

---

[← Domain 2](./domain-2.md) · [Next Domain →](./domain-4.md)
